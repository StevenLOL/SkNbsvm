{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBSVM Improvement and Test\n",
    "The Goal is to imporve the algorithm efficiency and adpt to sklearn standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from loadData import loadLabeled\n",
    "# Data contains the list of text and Class the corresponding Label (1=POS, 0=NEG)\n",
    "data, Class = loadLabeled('./train')\n",
    "\n",
    "from loadData import review_to_wordlist\n",
    "data_cleaned = []\n",
    "for i in xrange(len(data)):\n",
    "    data_cleaned.append(\" \".join(review_to_wordlist(data[i])))\n",
    "    \n",
    "from sklearn.cross_validation import train_test_split\n",
    "test_ratio = 0.4\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data_cleaned, Class, test_size = test_ratio, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(ngram_range=(1,2),binary=True)\n",
    "count_vect.fit(data_train)\n",
    "count_matrix = count_vect.transform(data_train)\n",
    "count_test = count_vect.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (15000,)\n",
      "Test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print \"Train: \",labels_train.shape\n",
    "print \"Test: \",labels_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Fit part optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = labels_train\n",
    "X = count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ..., 1 0 0]\n",
      "0.00120401382446\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "print 1*[y==1][0]\n",
    "print time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ..., 1 0 0]\n",
      "0.00124502182007\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "print [y==1][0].astype(int)\n",
    "print time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a - Version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.394751787186 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "alpha = 1.0\n",
    "nb_doc, voc_length = X.shape\n",
    "pos_idx = [y==1][0].astype(int)\n",
    "neg_idx = [y==0][0].astype(int)\n",
    "#Store the indicator vectors in sparse format to accelerate the computations\n",
    "pos_idx = sp.csr_matrix(pos_idx.T)\n",
    "neg_idx = sp.csr_matrix(neg_idx.T)\n",
    "#Use sparse format dot product to get a weightning vector stored in sparse format\n",
    "alpha_vec = sp.csr_matrix(alpha*np.ones(voc_length))\n",
    "p = (alpha_vec + pos_idx.dot(X)) \n",
    "norm_p = p.sum()\n",
    "p = p.multiply(1/norm_p)\n",
    "#print p.toarray()\n",
    "q = (alpha_vec + neg_idx.dot(X))\n",
    "norm_q = q.sum()\n",
    "q = q.multiply(1/norm_q)\n",
    "#print q.toarray()\n",
    "ratio = sp.csr_matrix(np.log((p.multiply(sp.csr_matrix(np.expand_dims(q.toarray()[0]**(-1),axis=0)))).data))\n",
    "print \"Time: \",time()-t0, \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a - Version 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import sparsefuncs\n",
    "from sklearn.utils.extmath import safe_sparse_dot, _fast_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0931611061096\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "pos_idx.dot(X)\n",
    "print time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0936689376831\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "safe_sparse_dot(neg_idx,X)\n",
    "print time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.399273872375 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "alpha = 1.0\n",
    "nb_doc, voc_length = X.shape\n",
    "pos_idx = [y==1][0].astype(int)\n",
    "neg_idx = [y==0][0].astype(int)\n",
    "#Store the indicator vectors in sparse format to accelerate the computations\n",
    "pos_idx = sp.csr_matrix(pos_idx.T)\n",
    "neg_idx = sp.csr_matrix(neg_idx.T)\n",
    "#Use sparse format dot product to get a weightning vector stored in sparse format\n",
    "alpha_vec = sp.csr_matrix(alpha*np.ones(voc_length))\n",
    "p = (alpha_vec + safe_sparse_dot(pos_idx,X)) \n",
    "norm_p = p.sum()\n",
    "p = p.multiply(1.0/norm_p)\n",
    "q = (alpha_vec + safe_sparse_dot(neg_idx,X))\n",
    "norm_q = q.sum()\n",
    "q = q.multiply(1.0/norm_q)\n",
    "inverseQ = sp.csr_matrix(np.expand_dims(q.toarray()[0]**(-1),axis=0))\n",
    "firstRatio = p.multiply(inverseQ).data\n",
    "ratio = sp.csr_matrix(np.log(firstRatio))\n",
    "print \"Time: \",time()-t0, \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.266833990282862"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.min(firstRatio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) NBSVM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import binarize\n",
    "import numpy as np\n",
    "    \n",
    "class NBmatrix(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, alpha, bina, n_jobs = 1):\n",
    "        self.alpha = alpha\n",
    "        self.bina = bina\n",
    "        self.n_jobs = n_jobs\n",
    "        self.r = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        alpha = self.alpha\n",
    "        nb_doc, voc_length = X.shape\n",
    "        pos_idx = [y==1][0].astype(int)\n",
    "        neg_idx = [y==0][0].astype(int)\n",
    "        #Store the indicator vectors in sparse format to accelerate the computations\n",
    "        pos_idx = sp.csr_matrix(pos_idx.T)\n",
    "        neg_idx = sp.csr_matrix(neg_idx.T)\n",
    "        #Use sparse format dot product to get a weightning vector stored in sparse format\n",
    "        alpha_vec = sp.csr_matrix(alpha*np.ones(voc_length))\n",
    "        p = (alpha_vec + pos_idx.dot(X)) \n",
    "        norm_p = p.sum()\n",
    "        p = p.multiply(1/norm_p)\n",
    "        #print p.toarray()\n",
    "        q = (alpha_vec + neg_idx.dot(X))\n",
    "        norm_q = q.sum()\n",
    "        q = q.multiply(1/norm_q)\n",
    "        #print q.toarray()\n",
    "        \n",
    "        ratio = sp.csr_matrix(np.log((p.multiply(sp.csr_matrix(np.expand_dims(q.toarray()[0]**(-1),axis=0)))).data))\n",
    "        #print ratio.toarray()\n",
    "        self.r = ratio #Stock the ratio vector to re-use it for transforming unlablled data\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #If the binarize option is set to true, we need now to recompute \"f\", our binarized word counter\n",
    "        if(self.bina == True):\n",
    "            f_hat = binarize(X, threshold = 0.0)\n",
    "        else :\n",
    "            f_hat = X\n",
    "        \n",
    "        f_tilde = f_hat.multiply(self.r)\n",
    "        return f_tilde\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
